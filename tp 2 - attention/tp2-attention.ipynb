{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" ### **Étape 1 : Configuration de l’environnement Kaggle**\n","metadata":{}},{"cell_type":"code","source":"# Installer les dépendances nécessaires\n!pip install torch torchvision transformers\n!pip install tensorboard\n!pip install nltk\n\n# Import des bibliothèques\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport pandas as pd\nimport numpy as np\nimport os\nfrom PIL import Image\nimport nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:01.107881Z","iopub.execute_input":"2025-12-25T10:36:01.108641Z","iopub.status.idle":"2025-12-25T10:36:10.593741Z","shell.execute_reply.started":"2025-12-25T10:36:01.108608Z","shell.execute_reply":"2025-12-25T10:36:10.592859Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.1rc0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\nRequirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\nRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\nRequirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### **Étape 2 : Télécharger et charger le dataset**","metadata":{}},{"cell_type":"code","source":"# Télécharger NLTK data pour tokenizer\nnltk.download('punkt')\n\n# Lire le fichier CSV des résultats\ndf = pd.read_csv('/kaggle/input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|')\ndf.columns = df.columns.str.strip()  # Nettoyer les noms de colonnes\ndf['comment'] = df['comment'].str.strip()  # Nettoyer les commentaires\n\nprint(\"Colonnes du dataset :\", df.columns)\nprint(df.head())\nprint(f\"Nombre total d'images : {df['image_name'].nunique()}\")\nprint(f\"Nombre total de légendes : {len(df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:10.595458Z","iopub.execute_input":"2025-12-25T10:36:10.595716Z","iopub.status.idle":"2025-12-25T10:36:10.967034Z","shell.execute_reply.started":"2025-12-25T10:36:10.595689Z","shell.execute_reply":"2025-12-25T10:36:10.966406Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Colonnes du dataset : Index(['image_name', 'comment_number', 'comment'], dtype='object')\n       image_name comment_number  \\\n0  1000092795.jpg              0   \n1  1000092795.jpg              1   \n2  1000092795.jpg              2   \n3  1000092795.jpg              3   \n4  1000092795.jpg              4   \n\n                                             comment  \n0  Two young guys with shaggy hair look at their ...  \n1  Two young , White males are outside near many ...  \n2   Two men in green shirts are standing in a yard .  \n3       A man in a blue shirt standing in a garden .  \n4            Two friends enjoy time spent together .  \nNombre total d'images : 31783\nNombre total de légendes : 158915\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"### **Étape 3 : Créer le vocabulaire**","metadata":{}},{"cell_type":"code","source":"# Étape 3 : Créer le vocabulaire (Corrigé)\nimport nltk\nfrom collections import Counter\nnltk.download('punkt')\n\ndef build_vocab(df, min_freq=2):\n    \"\"\"Construit le vocabulaire à partir des légendes\"\"\"\n    # Nettoyer les données : supprimer les commentaires vides ou NaN\n    df_clean = df[df['comment'].notna() & (df['comment'].str.strip() != '')]\n    \n    # Tokenizer les légendes\n    all_tokens = []\n    for caption in df_clean['comment']:\n        try:\n            tokens = nltk.word_tokenize(str(caption).lower())\n            all_tokens.extend(tokens)\n        except Exception as e:\n            print(f\"Erreur avec la légende : {caption}\")\n            continue\n    \n    # Compter les fréquences\n    word_counts = Counter(all_tokens)\n    \n    # Filtrer par fréquence minimale\n    vocab = ['<pad>', '<sos>', '<eos>', '<unk>']\n    vocab.extend([word for word, count in word_counts.items() if count >= min_freq])\n    \n    word2idx = {word: idx for idx, word in enumerate(vocab)}\n    idx2word = {idx: word for idx, word in enumerate(vocab)}\n    \n    print(f\"Taille du vocabulaire après filtrage (freq >= {min_freq}): {len(vocab)}\")\n    print(f\"10 mots les plus fréquents: {word_counts.most_common(10)}\")\n    \n    return vocab, word2idx, idx2word\n# Construire le vocabulaire\nvocab, word2idx, idx2word = build_vocab(df)\nvocab_size = len(vocab)\nprint(f\"Taille totale du vocabulaire : {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:10.967940Z","iopub.execute_input":"2025-12-25T10:36:10.968231Z","iopub.status.idle":"2025-12-25T10:36:18.450276Z","shell.execute_reply.started":"2025-12-25T10:36:10.968207Z","shell.execute_reply":"2025-12-25T10:36:18.449551Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Taille du vocabulaire après filtrage (freq >= 2): 12509\n10 mots les plus fréquents: [('a', 271704), ('.', 151065), ('in', 83466), ('the', 62978), ('on', 45669), ('and', 44263), ('man', 42598), ('is', 41116), ('of', 38776), ('with', 36207)]\nTaille totale du vocabulaire : 12509\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### **Étape 4 : Créer le Dataset personnalisé**","metadata":{}},{"cell_type":"code","source":"# Étape 4 : Créer le Dataset personnalisé (Corrigé)\nclass Flickr30kDataset(Dataset):\n    def __init__(self, df, image_dir, word2idx, transform=None, max_length=30):\n        \"\"\"\n        df: DataFrame contenant les légendes\n        image_dir: Chemin vers le dossier des images\n        word2idx: Dictionnaire de mapping mot -> index\n        transform: Transformations à appliquer aux images\n        max_length: Longueur maximale des légendes\n        \"\"\"\n        self.df = df\n        self.image_dir = image_dir\n        self.word2idx = word2idx\n        self.transform = transform\n        self.max_length = max_length\n        \n        # Nettoyer le DataFrame\n        self.df_clean = self.df[\n            self.df['comment'].notna() & \n            (self.df['comment'].str.strip() != '')\n        ].copy()\n        \n        # Grouper les légendes par image\n        self.image_to_captions = {}\n        for _, row in self.df_clean.iterrows():\n            img_name = row['image_name'].strip()\n            caption = str(row['comment']).strip()\n            \n            if img_name not in self.image_to_captions:\n                self.image_to_captions[img_name] = []\n            self.image_to_captions[img_name].append(caption)\n        \n        self.images = list(self.image_to_captions.keys())\n        print(f\"Nombre d'images dans le dataset : {len(self.images)}\")\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_name = self.images[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n        \n        # Vérifier si le fichier existe\n        if not os.path.exists(img_path):\n            # Essayer d'autres extensions\n            for ext in ['.jpg', '.jpeg', '.png']:\n                alt_path = img_path + ext\n                if os.path.exists(alt_path):\n                    img_path = alt_path\n                    break\n        \n        # Charger l'image\n        try:\n            image = Image.open(img_path).convert('RGB')\n        except Exception as e:\n            print(f\"Erreur de chargement de l'image {img_path}: {e}\")\n            # Retourner une image noire en cas d'erreur\n            image = Image.new('RGB', (224, 224), color='black')\n        \n        if self.transform:\n            image = self.transform(image)\n        \n        # Sélectionner une légende aléatoire pour cette image\n        captions = self.image_to_captions[img_name]\n        caption = np.random.choice(captions)\n        \n        # Tokenizer et encoder la légende\n        try:\n            tokens = nltk.word_tokenize(caption.lower())\n        except:\n            tokens = caption.lower().split()  # Fallback simple\n            \n        encoded = [self.word2idx['<sos>']]\n        \n        for token in tokens[:self.max_length-2]:  # -2 pour <sos> et <eos>\n            encoded.append(self.word2idx.get(token, self.word2idx['<unk>']))\n        \n        encoded.append(self.word2idx['<eos>'])\n        \n        # Padding\n        if len(encoded) < self.max_length:\n            encoded += [self.word2idx['<pad>']] * (self.max_length - len(encoded))\n        else:\n            encoded = encoded[:self.max_length]\n            encoded[-1] = self.word2idx['<eos>']\n        \n        return image, torch.tensor(encoded, dtype=torch.long)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:18.451360Z","iopub.execute_input":"2025-12-25T10:36:18.451674Z","iopub.status.idle":"2025-12-25T10:36:18.462344Z","shell.execute_reply.started":"2025-12-25T10:36:18.451640Z","shell.execute_reply":"2025-12-25T10:36:18.461599Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Étape 4 (suite) : Tester le dataset\n# Définir les transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.RandomCrop((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Créer le dataset complet\nimage_dir = '/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images'\ndataset = Flickr30kDataset(df, image_dir, word2idx, transform=transform)\n\n# Séparer en train/test (80/20)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\nprint(f\"Taille du dataset d'entraînement : {len(train_dataset)}\")\nprint(f\"Taille du dataset de test : {len(test_dataset)}\")\n\n# Tester un échantillon\nsample_image, sample_caption = dataset[0]\nprint(f\"Shape de l'image : {sample_image.shape}\")\nprint(f\"Shape de la légende : {sample_caption.shape}\")\nprint(f\"Légende encodée : {sample_caption[:10]}...\")  # Afficher les 10 premiers tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:18.463943Z","iopub.execute_input":"2025-12-25T10:36:18.464191Z","iopub.status.idle":"2025-12-25T10:36:23.892391Z","shell.execute_reply.started":"2025-12-25T10:36:18.464170Z","shell.execute_reply":"2025-12-25T10:36:23.891739Z"}},"outputs":[{"name":"stdout","text":"Nombre d'images dans le dataset : 31783\nTaille du dataset d'entraînement : 25426\nTaille du dataset de test : 6357\nShape de l'image : torch.Size([3, 224, 224])\nShape de la légende : torch.Size([30])\nLégende encodée : tensor([ 1, 33, 34, 17, 33, 35, 36, 32, 17, 33])...\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"### **Étape 5 : Créer les DataLoader avec fonction de collate**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\ndef collate_fn(batch):\n    \"\"\"Fonction pour grouper les batchs\"\"\"\n    images, captions = zip(*batch)\n    images = torch.stack(images, dim=0)\n    captions = torch.stack(captions, dim=0)\n    return images, captions\n\n# Créer les DataLoader\nbatch_size = 32  # Ajustez selon la mémoire disponible sur Kaggle\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    test_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    collate_fn=collate_fn,\n    num_workers=2\n)\n\n# Vérifier un batch\nimages, captions = next(iter(train_loader))\nprint(f\"Shape des images : {images.shape}\")  # Devrait être (batch, 3, 224, 224)\nprint(f\"Shape des légendes : {captions.shape}\")  # Devrait être (batch, max_length)\nprint(f\"Première légende : {captions[0][:10]}...\")  # Afficher les 10 premiers tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:23.893382Z","iopub.execute_input":"2025-12-25T10:36:23.893627Z","iopub.status.idle":"2025-12-25T10:36:24.789824Z","shell.execute_reply.started":"2025-12-25T10:36:23.893604Z","shell.execute_reply":"2025-12-25T10:36:24.788957Z"}},"outputs":[{"name":"stdout","text":"Shape des images : torch.Size([32, 3, 224, 224])\nShape des légendes : torch.Size([32, 30])\nPremière légende : tensor([  1,   4,  29,  21,  32,  55,  76, 942,  21, 238])...\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### **Étape 6 : Créer l'embedding layer**","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass EmbeddingLayer(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings=None):\n        super().__init__()\n        if pretrained_embeddings is not None:\n            self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n            # Initialisation Xavier\n            nn.init.xavier_uniform_(self.embedding.weight)\n    \n    def forward(self, x):\n        return self.embedding(x)\n\n# Créer l'embedding (pour l'instant aléatoire)\nembedding_dim = 300\nembedding_layer = EmbeddingLayer(vocab_size, embedding_dim)\nprint(\"Embedding layer créé\")\nprint(f\"Taille de l'embedding : {vocab_size} mots x {embedding_dim} dimensions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:25.793261Z","iopub.execute_input":"2025-12-25T10:36:25.793653Z","iopub.status.idle":"2025-12-25T10:36:25.847009Z","shell.execute_reply.started":"2025-12-25T10:36:25.793619Z","shell.execute_reply":"2025-12-25T10:36:25.846462Z"}},"outputs":[{"name":"stdout","text":"Embedding layer créé\nTaille de l'embedding : 12509 mots x 300 dimensions\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"### **Étape 7 : Extraire les features avec ResNet**","metadata":{}},{"cell_type":"code","source":"import torchvision.models as models\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Charger ResNet50 pré-entraîné\n        resnet = models.resnet50(pretrained=True)\n        \n        # Extraire les couches jusqu'à avant la dernière couche avgpool\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n        # Geler les poids\n        for param in self.resnet.parameters():\n            param.requires_grad = False\n        \n        # Adapter la taille\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((7, 7))\n        \n        # Mettre en mode évaluation\n        self.resnet.eval()\n        \n    def forward(self, images):\n        \"\"\"Extrait les features d'images\"\"\"\n        with torch.no_grad():\n            features = self.resnet(images)  # (batch, 2048, H, W)\n        \n        features = self.adaptive_pool(features)  # (batch, 2048, 7, 7)\n        features = features.permute(0, 2, 3, 1)  # (batch, 7, 7, 2048)\n        batch_size = features.size(0)\n        features = features.view(batch_size, -1, 2048)  # (batch, 49, 2048)\n        \n        return features\n\n# Tester l'extracteur\nfeature_extractor = FeatureExtractor()\nwith torch.no_grad():\n    test_features = feature_extractor(images[:2])\nprint(f\"Shape des features extraites : {test_features.shape}\")  # (2, 49, 2048)\nprint(f\"Nombre de features par image : {test_features.shape[1]}\")  # 49 (7x7)\nprint(f\"Dimension de chaque feature : {test_features.shape[2]}\")  # 2048","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:25.847802Z","iopub.execute_input":"2025-12-25T10:36:25.847989Z","iopub.status.idle":"2025-12-25T10:36:26.411365Z","shell.execute_reply.started":"2025-12-25T10:36:25.847970Z","shell.execute_reply":"2025-12-25T10:36:26.410589Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"Shape des features extraites : torch.Size([2, 49, 2048])\nNombre de features par image : 49\nDimension de chaque feature : 2048\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"### **Étape 8 : Implémenter le module d'attention**","metadata":{}},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(self, encoder_dim, decoder_dim):\n        super().__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n        \n        # Couche pour calculer les scores d'attention\n        self.attention_layer = nn.Linear(encoder_dim + decoder_dim, 1)\n        \n        # Softmax pour les poids d'attention\n        self.softmax = nn.Softmax(dim=1)\n        \n        # Initialisation des poids\n        nn.init.xavier_uniform_(self.attention_layer.weight)\n        nn.init.constant_(self.attention_layer.bias, 0)\n    \n    def forward(self, encoder_out, decoder_hidden):\n        \"\"\"\n        encoder_out: (batch_size, num_pixels, encoder_dim)\n        decoder_hidden: (batch_size, decoder_dim)\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        num_pixels = encoder_out.size(1)\n        \n        # Répéter decoder_hidden pour chaque pixel\n        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, num_pixels, 1)  # (batch, num_pixels, decoder_dim)\n        \n        # Concaténer features et hidden state\n        combined = torch.cat((encoder_out, decoder_hidden), dim=2)  # (batch, num_pixels, encoder_dim+decoder_dim)\n        \n        # Calculer les scores d'attention\n        attention_scores = self.attention_layer(combined).squeeze(2)  # (batch, num_pixels)\n        \n        # Appliquer softmax pour obtenir les poids\n        attention_weights = self.softmax(attention_scores)  # (batch, num_pixels)\n        \n        # Calculer le vecteur de contexte\n        context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_out)  # (batch, 1, encoder_dim)\n        context_vector = context_vector.squeeze(1)  # (batch, encoder_dim)\n        \n        return context_vector, attention_weights\n\n# Tester le module d'attention\nattention_dim = 512\nattention_module = Attention(encoder_dim=2048, decoder_dim=attention_dim)\n\n# Tester avec des features et un état caché fictifs\nbatch_size = 2\ndummy_features = torch.randn(batch_size, 49, 2048)\ndummy_hidden = torch.randn(batch_size, attention_dim)\n\ncontext_vector, attention_weights = attention_module(dummy_features, dummy_hidden)\nprint(f\"Shape du context vector : {context_vector.shape}\")  # (2, 2048)\nprint(f\"Shape des attention weights : {attention_weights.shape}\")  # (2, 49)\nprint(f\"Somme des poids d'attention : {attention_weights.sum(dim=1)}\")  # Devrait être ~1.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.412487Z","iopub.execute_input":"2025-12-25T10:36:26.412745Z","iopub.status.idle":"2025-12-25T10:36:26.424789Z","shell.execute_reply.started":"2025-12-25T10:36:26.412723Z","shell.execute_reply":"2025-12-25T10:36:26.423920Z"}},"outputs":[{"name":"stdout","text":"Shape du context vector : torch.Size([2, 2048])\nShape des attention weights : torch.Size([2, 49])\nSomme des poids d'attention : tensor([1.0000, 1.0000], grad_fn=<SumBackward1>)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"### **Étape 9 : Implémenter le LSTM avec attention**","metadata":{}},{"cell_type":"code","source":"class LSTMWithAttention(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, encoder_dim, vocab_size):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.encoder_dim = encoder_dim\n        self.vocab_size = vocab_size\n        \n        # Module d'attention\n        self.attention = Attention(encoder_dim, hidden_dim)\n        \n        # Portes du LSTM\n        self.lstm_cell = nn.LSTMCell(embedding_dim + encoder_dim, hidden_dim, bias=True)\n        \n        # Couche pour initialiser l'état caché\n        self.init_h = nn.Linear(encoder_dim, hidden_dim)\n        self.init_c = nn.Linear(encoder_dim, hidden_dim)\n        \n        # Couche pour prédire le mot suivant\n        self.fc = nn.Linear(hidden_dim, vocab_size)\n        \n        # Dropout\n        self.dropout = nn.Dropout(0.5)\n        \n        # Initialisation des poids\n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialisation des poids\"\"\"\n        nn.init.xavier_uniform_(self.init_h.weight)\n        nn.init.constant_(self.init_h.bias, 0)\n        nn.init.xavier_uniform_(self.init_c.weight)\n        nn.init.constant_(self.init_c.bias, 0)\n        nn.init.xavier_uniform_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n    \n    def init_hidden_state(self, encoder_out):\n        \"\"\"\n        Initialise les états cachés à partir des features de l'image\n        encoder_out: (batch_size, num_pixels, encoder_dim)\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)  # (batch_size, encoder_dim)\n        h = self.init_h(mean_encoder_out)  # (batch_size, hidden_dim)\n        c = self.init_c(mean_encoder_out)  # (batch_size, hidden_dim)\n        return h, c\n    \n    def forward(self, encoder_out, captions_embedded, teacher_forcing_ratio=0.5):\n        \"\"\"\n        encoder_out: (batch_size, num_pixels, encoder_dim)\n        captions_embedded: (batch_size, seq_len, embedding_dim)\n        teacher_forcing_ratio: probabilité d'utiliser teacher forcing\n        \"\"\"\n        batch_size = encoder_out.size(0)\n        seq_len = captions_embedded.size(1)\n        \n        # Initialiser les états cachés\n        h, c = self.init_hidden_state(encoder_out)\n        \n        # Créer des tenseurs pour stocker les prédictions\n        predictions = torch.zeros(batch_size, seq_len, self.vocab_size).to(encoder_out.device)\n        \n        # On ne peut pas utiliser teacher forcing de manière simple car nous n'avons pas \n        # accès à l'embedding layer ici. On va simplifier et toujours utiliser l'embedding\n        # du mot vrai (teacher forcing = 1.0)\n        \n        # Pour chaque pas de temps\n        for t in range(seq_len):\n            # Calculer l'attention\n            context_vector, _ = self.attention(encoder_out, h)\n            \n            # Utiliser l'embedding du mot vrai (simplification)\n            # Dans une version plus avancée, on pourrait passer l'embedding layer en paramètre\n            lstm_input = torch.cat([captions_embedded[:, t, :], context_vector], dim=1)\n            \n            # Mettre à jour les états LSTM\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            \n            # Prédiction du prochain mot\n            output = self.fc(self.dropout(h))\n            predictions[:, t, :] = output\n        \n        return predictions\n    \n    def decode_step(self, encoder_out, word_embedded, h, c):\n        \"\"\"Un pas de décodage pour l'inférence\"\"\"\n        # Calculer l'attention\n        context_vector, _ = self.attention(encoder_out, h)\n        \n        # Concaténer l'embedding du mot et le contexte\n        lstm_input = torch.cat([word_embedded, context_vector], dim=1)\n        \n        # Mettre à jour les états LSTM\n        h, c = self.lstm_cell(lstm_input, (h, c))\n        \n        # Prédiction du prochain mot\n        output = self.fc(self.dropout(h))\n        \n        return output, h, c","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.425627Z","iopub.execute_input":"2025-12-25T10:36:26.425817Z","iopub.status.idle":"2025-12-25T10:36:26.443265Z","shell.execute_reply.started":"2025-12-25T10:36:26.425797Z","shell.execute_reply":"2025-12-25T10:36:26.442463Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"### **Étape 10 : Modèle complet de captioning**","metadata":{}},{"cell_type":"code","source":"class ImageCaptioningModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        \n        # Feature extractor\n        self.encoder = FeatureExtractor()\n        encoder_dim = 2048\n        \n        # Embedding layer\n        self.embedding = EmbeddingLayer(vocab_size, embedding_dim)\n        \n        # LSTM with attention\n        self.decoder = LSTMWithAttention(embedding_dim, hidden_dim, encoder_dim, vocab_size)\n        \n    def forward(self, images, captions, teacher_forcing_ratio=0.5):\n        \"\"\"\n        images: (batch, 3, 224, 224)\n        captions: (batch, seq_len)\n        teacher_forcing_ratio: probabilité d'utiliser teacher forcing\n        \"\"\"\n        # Extraire les features\n        encoder_out = self.encoder(images)  # (batch, 49, 2048)\n        \n        # Embedding des captions\n        captions_embedded = self.embedding(captions)  # (batch, seq_len, embedding_dim)\n        \n        # Générer les prédictions avec le décodage\n        predictions = self.decoder(encoder_out, captions_embedded, teacher_forcing_ratio)\n        \n        return predictions\n    \n    def generate_caption(self, image, word2idx, idx2word, max_length=30):\n        \"\"\"Génère une légende pour une seule image\"\"\"\n        self.eval()\n        with torch.no_grad():\n            # Préparer l'image\n            if len(image.shape) == 3:\n                image = image.unsqueeze(0)  # (1, 3, 224, 224)\n            \n            # Extraire les features\n            encoder_out = self.encoder(image)  # (1, 49, 2048)\n            \n            # Initialiser les états cachés\n            h, c = self.decoder.init_hidden_state(encoder_out)\n            \n            # Commencer avec <sos>\n            input_word = torch.tensor([[word2idx['<sos>']]]).to(image.device)\n            \n            caption_words = []\n            attention_weights_list = []\n            \n            for _ in range(max_length):\n                # Embedding du mot courant\n                word_embedded = self.embedding(input_word).squeeze(1)\n                \n                # Un pas de décodage\n                output, h, c = self.decoder.decode_step(encoder_out, word_embedded, h, c)\n                \n                # Prédire le mot suivant\n                predicted_word = output.argmax(1)\n                word_idx = predicted_word.item()\n                \n                # Vérifier si c'est la fin\n                if word_idx == word2idx['<eos>']:\n                    break\n                \n                # Ajouter le mot à la légende\n                word = idx2word[word_idx]\n                if word not in ['<pad>', '<unk>']:\n                    caption_words.append(word)\n                \n                # Mettre à jour le mot d'entrée pour l'itération suivante\n                input_word = predicted_word.unsqueeze(1)\n            \n            return ' '.join(caption_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.444123Z","iopub.execute_input":"2025-12-25T10:36:26.444367Z","iopub.status.idle":"2025-12-25T10:36:26.461985Z","shell.execute_reply.started":"2025-12-25T10:36:26.444346Z","shell.execute_reply":"2025-12-25T10:36:26.461298Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### **Étape 11 : Initialiser le modèle et l'entraînement**","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Subset\nimport numpy as np\n\n# Reproductibilité\nnp.random.seed(42)\n\n# =========================\n# TRAIN : 50 %\n# =========================\ntrain_size = len(train_dataset)\ntrain_indices = np.random.choice(\n    train_size,\n    size=int(0.5 * train_size),\n    replace=False\n)\n\ntrain_subset = Subset(train_dataset, train_indices)\n\ntrain_loader = DataLoader(\n    train_subset,\n    batch_size=32,        # tu peux ajuster\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n\n# =========================\n# VALIDATION / TEST : 50 %\n# =========================\ntest_size = len(test_dataset)\ntest_indices = np.random.choice(\n    test_size,\n    size=int(0.5 * test_size),\n    replace=False\n)\n\ntest_subset = Subset(test_dataset, test_indices)\n\ntest_loader = DataLoader(\n    test_subset,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n# =========================\n# Vérification (IMPORTANT)\n# =========================\nprint(\"=== Vérification des données utilisées ===\")\nprint(f\"Train total       : {train_size}\")\nprint(f\"Train utilisé (50%): {len(train_loader.dataset)}\")\nprint(f\"Test total        : {test_size}\")\nprint(f\"Test utilisé (50%): {len(test_loader.dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.464523Z","iopub.execute_input":"2025-12-25T10:36:26.464741Z","iopub.status.idle":"2025-12-25T10:36:26.483495Z","shell.execute_reply.started":"2025-12-25T10:36:26.464720Z","shell.execute_reply":"2025-12-25T10:36:26.482644Z"}},"outputs":[{"name":"stdout","text":"=== Vérification des données utilisées ===\nTrain total       : 25426\nTrain utilisé (50%): 12713\nTest total        : 6357\nTest utilisé (50%): 3178\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Étape 11 : Initialiser le modèle et l'entraînement (version simplifiée)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Device: {device}\")\n\n# Hyperparamètres (version rapide)\nembedding_dim = 256     # bon compromis qualité / vitesse\nhidden_dim = 256        # rapide sur GPU\nlearning_rate = 0.001\nnum_epochs = 25\n         # ↓ suffisant pour observer la convergence\n\n\n# Créer le modèle\nmodel = ImageCaptioningModel(vocab_size, embedding_dim, hidden_dim).to(device)\nprint(f\"Modèle créé sur {device}\")\nprint(f\"Nombre de paramètres : {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Nombre de paramètres entraînables : {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# Fonction de perte et optimiseur\ncriterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=learning_rate\n)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n\n# Fonction pour décoder une séquence\ndef decode_sequence(sequence, idx2word):\n    \"\"\"Convertit une séquence d'indices en texte\"\"\"\n    words = []\n    for idx in sequence:\n        word = idx2word[idx]\n        if word == '<eos>':\n            break\n        if word not in ['<sos>', '<pad>', '<unk>']:\n            words.append(word)\n    return ' '.join(words)\n\n# Entraînement simplifié\nprint(\"Début de l'entraînement...\")\n\nfrom tqdm import tqdm\n\nfor epoch in range(num_epochs):\n    # Phase d'entraînement\n    model.train()\n    train_loss = 0.0\n    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n    \n    for images, captions in train_pbar:\n        images = images.to(device)\n        captions = captions.to(device)\n        \n        # Forward pass (sans teacher forcing)\n        outputs = model(images, captions)\n        \n        # Calcul de la loss\n        # On décale d'un pas de temps pour la prédiction\n        outputs = outputs[:, :-1, :].contiguous()  # Ignorer la dernière prédiction\n        targets = captions[:, 1:].contiguous()  # Ignorer <sos>\n        \n        loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        \n        # Gradient clipping pour éviter les explosions de gradient\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        train_loss += loss.item()\n        train_pbar.set_postfix({'loss': loss.item()})\n    \n    # Phase de validation\n    model.eval()\n    val_loss = 0.0\n    \n    with torch.no_grad():\n        val_pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n        for images, captions in val_pbar:\n            images = images.to(device)\n            captions = captions.to(device)\n            \n            outputs = model(images, captions)\n            outputs = outputs[:, :-1, :].contiguous()\n            targets = captions[:, 1:].contiguous()\n            \n            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n            val_loss += loss.item()\n            val_pbar.set_postfix({'loss': loss.item()})\n    \n    # Calcul des moyennes\n    avg_train_loss = train_loss / len(train_loader)\n    avg_val_loss = val_loss / len(test_loader)\n    \n    # Mise à jour du scheduler\n    scheduler.step()\n    \n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n    print(f\"  Val Loss: {avg_val_loss:.4f}\")\n    print(f\"  Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n    \n    # Générer un exemple de légende toutes les 5 époques\n    if (epoch + 1) % 5 == 0 or epoch == 0:\n        model.eval()\n        with torch.no_grad():\n            # Prendre une image de test\n            test_image, test_caption = test_dataset[0]\n            test_image_tensor = test_image.unsqueeze(0).to(device)\n            \n            # Générer une légende\n            generated_caption = model.generate_caption(test_image_tensor, word2idx, idx2word)\n            \n            # Décoder la vraie légende\n            true_caption = decode_sequence(test_caption.tolist(), idx2word)\n            \n            print(f\"\\nExemple de génération (Epoch {epoch+1}):\")\n            print(f\"  Vraie légende: {true_caption}\")\n            print(f\"  Légende générée: {generated_caption}\")\n            print(\"-\" * 60)\n\nprint(\"Entraînement terminé!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:36:26.484509Z","iopub.execute_input":"2025-12-25T10:36:26.484798Z","iopub.status.idle":"2025-12-25T11:13:20.716930Z","shell.execute_reply.started":"2025-12-25T10:36:26.484768Z","shell.execute_reply":"2025-12-25T11:13:20.716055Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nModèle créé sur cuda\nNombre de paramètres : 33,600,030\nNombre de paramètres entraînables : 10,091,998\nDébut de l'entraînement...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/25 [Train]: 100%|██████████| 398/398 [01:40<00:00,  3.95it/s, loss=4.53]\nEpoch 1/25 [Val]: 100%|██████████| 100/100 [00:21<00:00,  4.71it/s, loss=4.62]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/25\n  Train Loss: 5.1698\n  Val Loss: 4.3909\n  Learning Rate: 0.001000\n\nExemple de génération (Epoch 1):\n  Vraie légende: group of guys sitting in a circle .\n  Légende générée: a man in a blue shirt and a blue shirt and a blue shirt and a blue shirt .\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/25 [Train]: 100%|██████████| 398/398 [01:10<00:00,  5.65it/s, loss=4.06]\nEpoch 2/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.74it/s, loss=4.12]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/25\n  Train Loss: 4.3219\n  Val Loss: 4.1246\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.60it/s, loss=3.92]\nEpoch 3/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.82it/s, loss=4.04]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/25\n  Train Loss: 4.1275\n  Val Loss: 3.9869\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.59it/s, loss=3.94]\nEpoch 4/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.85it/s, loss=4.18]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/25\n  Train Loss: 3.9955\n  Val Loss: 3.8615\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.58it/s, loss=3.92]\nEpoch 5/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.85it/s, loss=3.93]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/25\n  Train Loss: 3.9106\n  Val Loss: 3.7847\n  Learning Rate: 0.001000\n\nExemple de génération (Epoch 5):\n  Vraie légende: four men sit on the ground next to a blue bag .\n  Légende générée: a man in a blue shirt is sitting on a sidewalk .\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.56it/s, loss=4.01]\nEpoch 6/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.77it/s, loss=3.89]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/25\n  Train Loss: 3.8434\n  Val Loss: 3.7056\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.55it/s, loss=3.69]\nEpoch 7/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.60it/s, loss=3.45]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/25\n  Train Loss: 3.7649\n  Val Loss: 3.6721\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.52it/s, loss=4.33]\nEpoch 8/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.45it/s, loss=3.6]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/25\n  Train Loss: 3.7343\n  Val Loss: 3.6355\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.52it/s, loss=3.97]\nEpoch 9/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.60it/s, loss=3.77]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/25\n  Train Loss: 3.6863\n  Val Loss: 3.6099\n  Learning Rate: 0.001000\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.54it/s, loss=3.48]\nEpoch 10/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.69it/s, loss=3.54]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/25\n  Train Loss: 3.6224\n  Val Loss: 3.5861\n  Learning Rate: 0.000500\n\nExemple de génération (Epoch 10):\n  Vraie légende: men sitting down in a circle .\n  Légende générée: a man in a blue shirt and a black shirt and a black shirt and a black shirt and a black shirt and a black shirt and a black shirt\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.57it/s, loss=4.29]\nEpoch 11/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.85it/s, loss=3.63]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/25\n  Train Loss: 3.5812\n  Val Loss: 3.5231\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/25 [Train]: 100%|██████████| 398/398 [01:11<00:00,  5.54it/s, loss=3.51]\nEpoch 12/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.57it/s, loss=3.07]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/25\n  Train Loss: 3.5556\n  Val Loss: 3.4902\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.45it/s, loss=3.67]\nEpoch 13/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.90it/s, loss=3.46]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/25\n  Train Loss: 3.5389\n  Val Loss: 3.4677\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.48it/s, loss=3.54]\nEpoch 14/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.60it/s, loss=3.48]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/25\n  Train Loss: 3.5143\n  Val Loss: 3.4768\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.47it/s, loss=3.26]\nEpoch 15/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.80it/s, loss=2.91]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/25\n  Train Loss: 3.4899\n  Val Loss: 3.4541\n  Learning Rate: 0.000500\n\nExemple de génération (Epoch 15):\n  Vraie légende: men sitting down in a circle .\n  Légende générée: a man in a blue shirt is sitting on a bench .\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/25 [Train]: 100%|██████████| 398/398 [01:13<00:00,  5.45it/s, loss=3.63]\nEpoch 16/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.71it/s, loss=3.5]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/25\n  Train Loss: 3.4713\n  Val Loss: 3.4610\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/25 [Train]: 100%|██████████| 398/398 [01:13<00:00,  5.44it/s, loss=3.57]\nEpoch 17/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.76it/s, loss=3.2]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/25\n  Train Loss: 3.4585\n  Val Loss: 3.4739\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.45it/s, loss=3.76]\nEpoch 18/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.64it/s, loss=3.72]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/25\n  Train Loss: 3.4452\n  Val Loss: 3.4371\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/25 [Train]: 100%|██████████| 398/398 [01:13<00:00,  5.43it/s, loss=3.55]\nEpoch 19/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.80it/s, loss=3.55]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/25\n  Train Loss: 3.4249\n  Val Loss: 3.4081\n  Learning Rate: 0.000500\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.47it/s, loss=3.54]\nEpoch 20/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.68it/s, loss=3.78]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/25\n  Train Loss: 3.4063\n  Val Loss: 3.4233\n  Learning Rate: 0.000250\n\nExemple de génération (Epoch 20):\n  Vraie légende: four boys sit on the street and talk .\n  Légende générée: a man in a blue shirt and a man in a blue shirt and a man in a blue shirt and a man in a blue shirt and a man\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.47it/s, loss=3.2] \nEpoch 21/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.67it/s, loss=3.85]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21/25\n  Train Loss: 3.3871\n  Val Loss: 3.4412\n  Learning Rate: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.48it/s, loss=3.76]\nEpoch 22/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.89it/s, loss=3.6]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22/25\n  Train Loss: 3.3664\n  Val Loss: 3.4335\n  Learning Rate: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.48it/s, loss=3.54]\nEpoch 23/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.52it/s, loss=3.41]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23/25\n  Train Loss: 3.3659\n  Val Loss: 3.4022\n  Learning Rate: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.47it/s, loss=3.5] \nEpoch 24/25 [Val]: 100%|██████████| 100/100 [00:14<00:00,  6.71it/s, loss=3.63]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24/25\n  Train Loss: 3.3570\n  Val Loss: 3.3960\n  Learning Rate: 0.000250\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/25 [Train]: 100%|██████████| 398/398 [01:12<00:00,  5.46it/s, loss=2.89]\nEpoch 25/25 [Val]: 100%|██████████| 100/100 [00:15<00:00,  6.66it/s, loss=3.32]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25/25\n  Train Loss: 3.3439\n  Val Loss: 3.3674\n  Learning Rate: 0.000250\n\nExemple de génération (Epoch 25):\n  Vraie légende: group of guys sitting in a circle .\n  Légende générée: a man in a blue shirt and a black shirt is sitting on a bench .\n------------------------------------------------------------\nEntraînement terminé!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"### **Étape 12 : Évaluation du modèle**","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion, device, max_examples=10):\n    \"\"\"Évalue le modèle sur un dataloader\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_examples = 0\n    \n    with torch.no_grad():\n        for i, (images, captions) in enumerate(dataloader):\n            if i * batch_size >= max_examples:\n                break\n                \n            images = images.to(device)\n            captions = captions.to(device)\n            \n            # Forward pass\n            outputs = model(images, captions, teacher_forcing_ratio=0.0)\n            outputs = outputs[:, :-1, :].contiguous()\n            targets = captions[:, 1:].contiguous()\n            \n            # Calcul de la loss\n            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n            \n            total_loss += loss.item() * images.size(0)\n            total_examples += images.size(0)\n            \n            # Afficher quelques exemples\n            if i == 0:\n                print(\"\\nExemples de génération sur le test set:\")\n                for j in range(min(3, images.size(0))):\n                    # Générer une légende\n                    generated = model.generate_caption(images[j:j+1], word2idx, idx2word)\n                    \n                    # Décoder la vraie légende\n                    true_caption = decode_sequence(captions[j].tolist(), idx2word)\n                    \n                    print(f\"\\nExemple {j+1}:\")\n                    print(f\"  Vraie: {true_caption}\")\n                    print(f\"  Générée: {generated}\")\n    \n    avg_loss = total_loss / total_examples if total_examples > 0 else 0\n    print(f\"\\nÉvaluation terminée\")\n    print(f\"Loss moyenne sur {total_examples} exemples: {avg_loss:.4f}\")\n    \n    return avg_loss\n\n# Évaluer le modèle\nprint(\"Évaluation du modèle sur le test set...\")\ntest_loss = evaluate_model(model, test_loader, criterion, device, max_examples=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:13:20.718203Z","iopub.execute_input":"2025-12-25T11:13:20.718477Z","iopub.status.idle":"2025-12-25T11:13:23.648705Z","shell.execute_reply.started":"2025-12-25T11:13:20.718447Z","shell.execute_reply":"2025-12-25T11:13:23.647882Z"}},"outputs":[{"name":"stdout","text":"Évaluation du modèle sur le test set...\n\nExemples de génération sur le test set:\n\nExemple 1:\n  Vraie: a man is working in a small store with his cat .\n  Générée: a man in a black shirt and a black shirt is sitting on a table .\n\nExemple 2:\n  Vraie: kids are resting in the green outdoors .\n  Générée: a group of people are sitting on a bench .\n\nExemple 3:\n  Vraie: a man with a hat is smoking a cigarette in front of another person and a body of water can be seen reflecting a building in the background\n  Générée: a man in a blue shirt and a blue shirt and a blue shirt and a blue shirt and a blue shirt and a blue shirt and a blue shirt\n\nÉvaluation terminée\nLoss moyenne sur 512 exemples: 3.3982\n","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"### **Étape 13 : Sauvegarde et chargement du modèle**","metadata":{}},{"cell_type":"code","source":"import os\nimport datetime\n\ndef save_model(model, optimizer, epoch, word2idx, idx2word, path='model_checkpoint.pth'):\n    \"\"\"Sauvegarde le modèle\"\"\"\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'vocab_size': vocab_size,\n        'embedding_dim': embedding_dim,\n        'hidden_dim': hidden_dim,\n        'word2idx': word2idx,\n        'idx2word': idx2word,\n        'loss': test_loss if 'test_loss' in locals() else None\n    }\n    \n    # Créer un nom de fichier avec la date\n    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    filename = f'caption_model_{timestamp}.pth'\n    \n    torch.save(checkpoint, filename)\n    print(f\"Modèle sauvegardé dans {filename}\")\n    \n    return filename\n\ndef load_model(filename, device='cuda'):\n    \"\"\"Charge un modèle sauvegardé\"\"\"\n    checkpoint = torch.load(filename, map_location=device)\n    \n    # Créer le modèle\n    model = ImageCaptioningModel(\n        checkpoint['vocab_size'],\n        checkpoint['embedding_dim'],\n        checkpoint['hidden_dim']\n    ).to(device)\n    \n    # Charger les poids\n    model.load_state_dict(checkpoint['model_state_dict'])\n    \n    # Charger l'optimizer si nécessaire\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    \n    print(f\"Modèle chargé depuis {filename}\")\n    print(f\"Époque: {checkpoint['epoch']}, Loss: {checkpoint.get('loss', 'N/A')}\")\n    \n    return model, optimizer, checkpoint\n\n# Sauvegarder le modèle\nsaved_file = save_model(model, optimizer, num_epochs, word2idx, idx2word)\nprint(f\"Modèle sauvegardé: {saved_file}\")\n\n# Pour charger le modèle plus tard:\n# loaded_model, loaded_optimizer, checkpoint = load_model('votre_fichier.pth', device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:13:23.649802Z","iopub.execute_input":"2025-12-25T11:13:23.650027Z","iopub.status.idle":"2025-12-25T11:13:23.976861Z","shell.execute_reply.started":"2025-12-25T11:13:23.650001Z","shell.execute_reply":"2025-12-25T11:13:23.976223Z"}},"outputs":[{"name":"stdout","text":"Modèle sauvegardé dans caption_model_20251225_111323.pth\nModèle sauvegardé: caption_model_20251225_111323.pth\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"### **Étape 14 : Génération interactive**","metadata":{}},{"cell_type":"code","source":"# Définir d'abord la fonction generate_caption_beam_search\ndef generate_caption_beam_search(model, image, word2idx, idx2word, beam_size=3, max_length=20):\n    \"\"\"Génère une légende avec beam search\"\"\"\n    model.eval()\n    \n    # Préparer l'image\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    \n    image = image.to(device)\n    \n    with torch.no_grad():\n        # Extraire les features (méthode dépend du modèle)\n        if hasattr(model, 'encoder'):\n            # Pour les modèles avec encoder\n            features = model.encoder(image)\n            if hasattr(model, 'feature_adapter'):\n                # Adapter les features si nécessaire\n                batch_size = features.size(0)\n                features = features.view(batch_size, -1)\n                features = model.feature_adapter(features)\n        else:\n            # Méthode générique\n            features = image\n    \n    # Initialiser les beams\n    beams = [{\n        'sequence': [word2idx['<sos>']],\n        'score': 0.0,\n        'features': features.clone() if features is not None else None\n    }]\n    \n    for step in range(max_length):\n        new_beams = []\n        \n        for beam in beams:\n            # Si la séquence se termine par <eos>, la garder telle quelle\n            if beam['sequence'][-1] == word2idx['<eos>']:\n                new_beams.append(beam)\n                continue\n            \n            # Préparer l'entrée pour le modèle\n            # Note: Cette partie dépend de la structure de votre modèle\n            # Vous devrez peut-être l'adapter\n            \n            # Pour les modèles simples\n            try:\n                # Tenter d'utiliser la méthode generate_caption_step si elle existe\n                if hasattr(model, 'generate_caption_step'):\n                    # Vous devriez implémenter cette méthode dans votre modèle\n                    pass\n                else:\n                    # Approche simplifiée: utiliser la méthode forward du modèle\n                    # Cette partie est complexe et dépend de l'architecture\n                    # Pour l'instant, nous allons utiliser une approche plus simple\n                    continue\n            except:\n                continue\n        \n        # Trier et garder les beam_size meilleurs beams\n        beams = sorted(new_beams, key=lambda x: x['score'], reverse=True)[:beam_size]\n        \n        # Si tous les beams se terminent par <eos>, arrêter\n        if all(beam['sequence'][-1] == word2idx['<eos>'] for beam in beams):\n            break\n    \n    # Meilleure séquence\n    best_beam = beams[0]\n    \n    # Convertir en texte\n    words = []\n    for idx in best_beam['sequence'][1:]:  # Ignorer <sos>\n        if idx == word2idx['<eos>']:\n            break\n        word = idx2word[idx]\n        if word not in ['<pad>', '<unk>']:\n            words.append(word)\n    \n    return ' '.join(words)\n\n# Version simplifiée sans beam search pour l'instant\ndef generate_caption_simple(model, image, word2idx, idx2word, max_length=20):\n    \"\"\"Génère une légende simple (sans beam search)\"\"\"\n    model.eval()\n    \n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    \n    image = image.to(device)\n    \n    # Utiliser la méthode generate_caption si elle existe\n    if hasattr(model, 'generate_caption'):\n        return model.generate_caption(image, word2idx, idx2word, max_length)\n    else:\n        # Fallback: méthode générique simple\n        with torch.no_grad():\n            # Cette partie dépend de votre modèle\n            # Pour l'instant, retourner une légende par défaut\n            return \"a person in an image\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:47:29.505902Z","iopub.execute_input":"2025-12-25T11:47:29.506537Z","iopub.status.idle":"2025-12-25T11:47:29.517323Z","shell.execute_reply.started":"2025-12-25T11:47:29.506505Z","shell.execute_reply":"2025-12-25T11:47:29.516711Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Exemple d'utilisation interactive\nprint(\"\\nGénération interactive:\")\nprint(\"=\" * 50)\n\n# Assurez-vous que le modèle est en mode évaluation\nmodel.eval()\n\n# Prendre quelques images au hasard du test set\nimport random\n\nnum_samples = 5\nsample_indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))\n\nfor i, idx in enumerate(sample_indices):\n    test_image, test_caption = test_dataset[idx]\n    \n    # Vérifier si le modèle a une méthode generate_caption\n    if hasattr(model, 'generate_caption'):\n        # Générer une légende avec la méthode du modèle\n        generated = model.generate_caption(test_image.unsqueeze(0).to(device), word2idx, idx2word)\n    else:\n        # Fallback: utiliser la méthode simple\n        generated = generate_caption_simple(model, test_image, word2idx, idx2word)\n    \n    # Décoder la vraie légende\n    true_caption = decode_sequence(test_caption.tolist(), idx2word)\n    \n    print(f\"\\nExemple {i+1}:\")\n    print(f\"  Vraie légende: {true_caption}\")\n    print(f\"  Légende générée: {generated}\")\n    \n    print(\"-\" * 50)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"GÉNÉRATION INTERACTIVE TERMINÉE\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:48:11.473442Z","iopub.execute_input":"2025-12-25T11:48:11.474167Z","iopub.status.idle":"2025-12-25T11:48:11.655841Z","shell.execute_reply.started":"2025-12-25T11:48:11.474138Z","shell.execute_reply":"2025-12-25T11:48:11.655174Z"}},"outputs":[{"name":"stdout","text":"\nGénération interactive:\n==================================================\n\nExemple 1:\n  Vraie légende: a clown making a balloon animal for a pretty lady .\n  Légende générée: a woman in a red shirt and a red shirt is standing on a street .\n--------------------------------------------------\n\nExemple 2:\n  Vraie légende: two people on a street ; one sitting on the planter surrounding a tree .\n  Légende générée: a man in a black shirt is sitting on a bench .\n--------------------------------------------------\n\nExemple 3:\n  Vraie légende: young girl wearing two piece black bathing suit running in the water with a smile on her face .\n  Légende générée: a young boy in a blue shirt is standing in the water .\n--------------------------------------------------\n\nExemple 4:\n  Vraie légende: two guys are standing in front of a garage door looking at each other talking .\n  Légende générée: a man in a black shirt and a black shirt and a black shirt and a black shirt and a black shirt and a black shirt and a black shirt\n--------------------------------------------------\n\nExemple 5:\n  Vraie légende: a woman and a man sit on a bench against a wall with somber expressions .\n  Légende générée: a man in a red shirt and a black shirt and a black shirt and a black shirt and a black shirt and a black shirt and a black shirt\n--------------------------------------------------\n\n============================================================\nGÉNÉRATION INTERACTIVE TERMINÉE\n============================================================\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"### **Étape 15 : Visualisation de l'attention**","metadata":{}},{"cell_type":"code","source":"def visualize_attention(model, image, word2idx, idx2word, max_length=30):\n    \"\"\"Génère une légende et visualise les poids d'attention\"\"\"\n    model.eval()\n    \n    # Préparer l'image\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    \n    image = image.to(device)\n    \n    with torch.no_grad():\n        # Extraire les features\n        encoder_out = model.encoder(image)  # (1, 49, 2048)\n        \n        # Initialiser les états cachés\n        h, c = model.decoder.init_hidden_state(encoder_out)\n        \n        # Commencer avec <sos>\n        input_word = torch.tensor([[word2idx['<sos>']]]).to(device)\n        \n        caption_words = []\n        attention_weights_list = []\n        \n        for _ in range(max_length):\n            # Embedding du mot courant\n            word_embedded = model.embedding(input_word).squeeze(1)\n            \n            # Calculer l'attention (nous avons besoin d'accéder aux poids)\n            batch_size = encoder_out.size(0)\n            num_pixels = encoder_out.size(1)\n            \n            # Répéter decoder_hidden pour chaque pixel\n            decoder_hidden = h.unsqueeze(1).repeat(1, num_pixels, 1)\n            \n            # Concaténer features et hidden state\n            combined = torch.cat((encoder_out, decoder_hidden), dim=2)\n            \n            # Calculer les scores d'attention\n            attention_scores = model.decoder.attention.attention_layer(combined).squeeze(2)\n            attention_weights = model.decoder.attention.softmax(attention_scores)\n            \n            # Calculer le vecteur de contexte\n            context_vector = torch.bmm(attention_weights.unsqueeze(1), encoder_out).squeeze(1)\n            \n            # Stocker les poids d'attention\n            attention_weights_list.append(attention_weights.squeeze(0).cpu().numpy())\n            \n            # Un pas de décodage\n            lstm_input = torch.cat([word_embedded, context_vector], dim=1)\n            h, c = model.decoder.lstm_cell(lstm_input, (h, c))\n            \n            # Prédire le mot suivant\n            output = model.decoder.fc(model.decoder.dropout(h))\n            predicted_word = output.argmax(1)\n            word_idx = predicted_word.item()\n            \n            # Vérifier si c'est la fin\n            if word_idx == word2idx['<eos>']:\n                break\n            \n            # Ajouter le mot à la légende\n            word = idx2word[word_idx]\n            if word not in ['<pad>', '<unk>']:\n                caption_words.append(word)\n            \n            # Mettre à jour le mot d'entrée\n            input_word = predicted_word.unsqueeze(1)\n        \n        caption = ' '.join(caption_words)\n        attention_weights_array = np.array(attention_weights_list)  # (num_words, 49)\n        \n        return caption, attention_weights_array\n\n# Tester la visualisation de l'attention\nprint(\"\\nVisualisation de l'attention:\")\ntest_image, test_caption = test_dataset[0]\ncaption, attention_weights = visualize_attention(model, test_image, word2idx, idx2word)\n\nprint(f\"Légende générée: {caption}\")\nprint(f\"Shape des poids d'attention: {attention_weights.shape}\")\nprint(f\"Poids d'attention pour le premier mot: {attention_weights[0][:10]}...\")  # Afficher les 10 premiers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:48:18.535705Z","iopub.execute_input":"2025-12-25T11:48:18.536304Z","iopub.status.idle":"2025-12-25T11:48:18.574698Z","shell.execute_reply.started":"2025-12-25T11:48:18.536267Z","shell.execute_reply":"2025-12-25T11:48:18.574111Z"}},"outputs":[{"name":"stdout","text":"\nVisualisation de l'attention:\nLégende générée: a man in a blue shirt and a blue shirt is sitting on a bench .\nShape des poids d'attention: (17, 49)\nPoids d'attention pour le premier mot: [0.0059053  0.04198002 0.05120409 0.02695383 0.01071807 0.00330091\n 0.00296614 0.02867693 0.03618455 0.02395026]...\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"### **Résumé final et conseils pour Kaggle**","metadata":{}},{"cell_type":"code","source":"print(\"\\n\" + \"=\"*60)\nprint(\"RÉSUMÉ DU TP - IMAGE CAPTIONING AVEC ATTENTION\")\nprint(\"=\"*60)\nprint(f\"✔ Dataset: Flickr30k ({len(dataset)} images)\")\nprint(f\"✔ Vocabulaire: {vocab_size} mots\")\nprint(f\"✔ Modèle: ResNet50 + LSTM avec Attention\")\nprint(f\"✔ Embedding dimension: {embedding_dim}\")\nprint(f\"✔ Hidden dimension: {hidden_dim}\")\nprint(f\"✔ Batch size: {batch_size}\")\nprint(f\"✔ Époques d'entraînement: {num_epochs}\")\nprint(f\"✔ Device utilisé: {device}\")\nprint(\"=\"*60)\nprint(\"\\nConseils pour Kaggle:\")\nprint(\"1. Commencez avec peu d'époques (10-20) pour tester\")\nprint(\"2. Ajustez batch_size selon la mémoire disponible\")\nprint(\"3. Utilisez le gradient clipping pour stabiliser l'entraînement\")\nprint(\"4. Sauvegardez régulièrement votre modèle\")\nprint(\"5. Testez avec beam search pour de meilleurs résultats\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T11:48:24.317736Z","iopub.execute_input":"2025-12-25T11:48:24.318451Z","iopub.status.idle":"2025-12-25T11:48:24.324625Z","shell.execute_reply.started":"2025-12-25T11:48:24.318421Z","shell.execute_reply":"2025-12-25T11:48:24.323896Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nRÉSUMÉ DU TP - IMAGE CAPTIONING AVEC ATTENTION\n============================================================\n✔ Dataset: Flickr30k (31783 images)\n✔ Vocabulaire: 12509 mots\n✔ Modèle: ResNet50 + LSTM avec Attention\n✔ Embedding dimension: 256\n✔ Hidden dimension: 256\n✔ Batch size: 2\n✔ Époques d'entraînement: 25\n✔ Device utilisé: cuda\n============================================================\n\nConseils pour Kaggle:\n1. Commencez avec peu d'époques (10-20) pour tester\n2. Ajustez batch_size selon la mémoire disponible\n3. Utilisez le gradient clipping pour stabiliser l'entraînement\n4. Sauvegardez régulièrement votre modèle\n5. Testez avec beam search pour de meilleurs résultats\n============================================================\n","output_type":"stream"}],"execution_count":38}]}