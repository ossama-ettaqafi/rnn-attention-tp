# **Compte Rendu du TP : Image Captioning avec Attention**

## **I. Introduction**
Ce TP a pour objectif d‚Äôimpl√©menter un mod√®le de *g√©n√©ration automatique de l√©gendes d‚Äôimages* (image captioning) en combinant :
- Un **encodeur visuel** bas√© sur ResNet50 pr√©-entra√Æn√© (transfer learning),
- Un **d√©codeur s√©quentiel** de type LSTM,
- Un **m√©canisme d‚Äôattention** permettant au mod√®le de se concentrer sur diff√©rentes r√©gions de l‚Äôimage lors de la g√©n√©ration de chaque mot.

Le dataset utilis√© est **Flickr30k**, contenant 31 783 images et 158 915 l√©gendes.

---

## **II. Structure du projet r√©alis√©**

### **1. Pr√©paration de l‚Äôenvironnement**
- Installation des biblioth√®ques n√©cessaires : `torch`, `torchvision`, `transformers`, `tensorboard`, `nltk`.
- V√©rification de la disponibilit√© du GPU (CUDA).

### **2. Chargement et pr√©traitement des donn√©es**
- Lecture du fichier CSV `results.csv`.
- Nettoyage des colonnes et des l√©gendes.
- **Tokenisation** des l√©gendes avec NLTK (`word_tokenize`).
- Construction d‚Äôun vocabulaire de **12 509 mots** (fr√©quence minimale = 2).
- Ajout des tokens sp√©ciaux : `<pad>`, `<sos>`, `<eos>`, `<unk>`.

### **3. Cr√©ation du Dataset personnalis√©**
- Classe `Flickr30kDataset` pour charger images et l√©gendes.
- Transformations appliqu√©es aux images :
  - Redimensionnement √† 256x256 ‚Üí RandomCrop 224x224 ‚Üí RandomHorizontalFlip ‚Üí Normalisation (ImageNet).
- Encodage des l√©gendes avec padding (longueur maximale = 30).

### **4. Extraction des caract√©ristiques visuelles**
- **ResNet50** pr√©-entra√Æn√©, gel√© (`requires_grad = False`).
- Extraction des features apr√®s la derni√®re couche convolutionnelle (`avgpool`).
- Format de sortie : `(batch, 49, 2048)` (49 r√©gions spatiales de 2048 dimensions).

### **5. Module d‚Äôattention**
- Impl√©mentation de la classe `Attention`.
- Calcul des scores d‚Äôattention par combinaison lin√©aire des features visuelles et de l‚Äô√©tat cach√© du LSTM.
- Softmax pour obtenir les poids ‚Üí produit avec les features pour obtenir un **vecteur de contexte**.

### **6. LSTM avec m√©canisme d‚Äôattention**
- Classe `LSTMWithAttention` utilisant `nn.LSTMCell`.
- √Ä chaque pas de temps :
  - Calcul du vecteur de contexte via le module d‚Äôattention.
  - Concat√©nation avec l‚Äôembedding du mot courant.
  - Mise √† jour des √©tats cach√©s (h, c).

### **7. Mod√®le complet**
- Classe `ImageCaptioningModel` int√©grant :
  - `FeatureExtractor` (ResNet50)
  - `EmbeddingLayer`
  - `LSTMWithAttention`
- M√©thode `generate_caption` pour l‚Äôinf√©rence.

### **8. Entra√Ænement**
- **Hyperparam√®tres** :
  - Taille des embeddings : 256
  - Taille cach√©e du LSTM : 256
  - Taille de batch : 32
  - Nombre d‚Äô√©poques : 25
  - Optimiseur : Adam (LR = 0.001)
  - Scheduler : StepLR (step=10, gamma=0.5)
  - Fonction de perte : `CrossEntropyLoss` (ignorant `<pad>`)
- **Teacher forcing** : utilisation de la l√©gende r√©elle comme entr√©e √† chaque pas.
- **Gradient clipping** (max_norm = 1.0) pour √©viter l‚Äôexplosion des gradients.

### **9. √âvaluation et r√©sultats**
- **Loss finale** :
  - Train : 3.34
  - Validation : 3.37
- **Exemples de g√©n√©ration** :
  - Le mod√®le apprend √† d√©crire des sc√®nes simples ("a man in a blue shirt", "group of people sitting").
  - Des r√©p√©titions persistent ("blue shirt and a blue shirt...").
  - La s√©mantique g√©n√©rale est correcte, mais la diversit√© lexicale est limit√©e.

### **10. Sauvegarde et chargement**
- Fonctions `save_model` et `load_model` pour persister le mod√®le entra√Æn√©.
- Fichier de checkpoint incluant :
  - Poids du mod√®le et de l‚Äôoptimiseur
  - Vocabulaire (`word2idx`, `idx2word`)
  - Hyperparam√®tres

### **11. Visualisation de l‚Äôattention**
- Fonction `visualize_attention` pour extraire les poids d‚Äôattention √† chaque mot.
- Permet de comprendre sur quelles r√©gions de l‚Äôimage le mod√®le se focalise.

---

## **III. Points forts du TP**
‚úÖ **Architecture moderne** : Combinaison ResNet + LSTM + Attention.  
‚úÖ **Gestion efficace du vocabulaire** avec tokens sp√©ciaux et padding.  
‚úÖ **Utilisation de TensorBoard** pour le suivi des courbes de loss.  
‚úÖ **Gradient clipping** et **scheduling du LR** pour stabiliser l‚Äôapprentissage.  
‚úÖ **G√©n√©ration interactive** avec possibilit√© de visualisation de l‚Äôattention.  

---

## **IV. Difficult√©s rencontr√©es**
‚ö†Ô∏è **R√©p√©titions dans les l√©gendes g√©n√©r√©es** ‚Üí probl√®me classique des mod√®les s√©quentiels.  
‚ö†Ô∏è **Temps d‚Äôentra√Ænement long** (~37 minutes pour 25 √©poques sur GPU T4).  
‚ö†Ô∏è **M√©moire GPU limit√©e** sur Kaggle ‚Üí r√©duction de la taille du batch.  
‚ö†Ô∏è **Beam search non impl√©ment√©** dans la version finale ‚Üí g√©n√©ration d√©terministe (argmax).  

---

## **V. Am√©liorations possibles**
üîπ **Beam search** pour am√©liorer la qualit√© des l√©gendes g√©n√©r√©es.  
üîπ **Fine-tuning partiel** du ResNet apr√®s quelques √©poques.  
üîπ **Utilisation d‚Äôembeddings pr√©-entra√Æn√©s** (Word2Vec, GloVe).  
üîπ **Ajout de r√©gularisation** (dropout plus fort, weight decay).  
üîπ **√âvaluation quantitative** avec m√©triques BLEU, METEOR, CIDEr.  
üîπ **Augmentation de donn√©es** plus pouss√©e (rotation, changement de couleur).  

---

## **VI. Conclusion**
Ce TP a permis de mettre en ≈ìuvre un pipeline complet d‚Äôimage captioning, depuis le chargement des donn√©es jusqu‚Äô√† la g√©n√©ration de l√©gendes avec visualisation de l‚Äôattention. Le mod√®le appris, bien que perfectible, produit des descriptions coh√©rentes pour des images simples. Les concepts cl√©s ma√Ætris√©s sont :
- Transfer learning avec ResNet
- M√©canismes d‚Äôattention
- Gestion de s√©quences avec LSTM
- Entra√Ænement de mod√®les multimodaux (image + texte)

**Prochaine √©tape** : explorer des architectures plus r√©centes (Transformer, Vision Transformer) et des techniques avanc√©es comme le self-critical sequence training.